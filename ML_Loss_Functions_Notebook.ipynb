{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ML Loss Functions Notebook\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, hinge_loss, mean_squared_error, mean_absolute_error\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Regression dataset\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=1, noise=20, random_state=42)\n",
    "y_pred_reg = y_reg + np.random.normal(0, 10, size=len(y_reg))\n",
    "\n",
    "mse = mean_squared_error(y_reg, y_pred_reg)\n",
    "mae = mean_absolute_error(y_reg, y_pred_reg)\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    is_small = np.abs(error) <= delta\n",
    "    small_loss = 0.5 * (error ** 2)\n",
    "    big_loss = delta * (np.abs(error) - 0.5 * delta)\n",
    "    return np.where(is_small, small_loss, big_loss).mean()\n",
    "\n",
    "huber = huber_loss(y_reg, y_pred_reg)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"Huber Loss:\", huber)\n",
    "\n",
    "# Classification dataset\n",
    "X_clf, y_clf = make_classification(n_samples=200, n_features=4, random_state=42)\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_clf, y_clf)\n",
    "probs = log_model.predict_proba(X_clf)\n",
    "print(\"Log Loss:\", log_loss(y_clf, probs))\n",
    "\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_clf, y_clf)\n",
    "decision = svm.decision_function(X_clf)\n",
    "print(\"Hinge Loss:\", hinge_loss(y_clf, decision))\n",
    "\n",
    "def squared_hinge_loss(y_true, y_pred):\n",
    "    y_true = np.where(y_true == 0, -1, 1)\n",
    "    losses = np.maximum(0, 1 - y_true * y_pred)\n",
    "    return np.mean(losses ** 2)\n",
    "\n",
    "print(\"Squared Hinge Loss:\", squared_hinge_loss(y_clf, decision))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb46277",
   "metadata": {},
   "source": [
    "**ML Loss Functions Explained & Implemented**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORTS\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, hinge_loss, mean_squared_error, mean_absolute_error\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc60366",
   "metadata": {},
   "source": [
    "**2. Regression Loss Functions Example (MSE, MAE, Huber)**\n",
    "\n",
    "***Create a regression dataset:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59baf6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=200, n_features=1, noise=20, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60127c71",
   "metadata": {},
   "source": [
    "***Predictions (simulate model output)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_reg = y_reg + np.random.normal(0, 10, size=len(y_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88556dbf",
   "metadata": {},
   "source": [
    "**2.1 MSE — Mean Squared Error**\n",
    "\n",
    "\n",
    "***Used for: Standard regression***\n",
    "\n",
    "***Why: Punishes large errors strongly → good for clean data.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c1d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_reg, y_pred_reg)\n",
    "print(\"MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b0de9",
   "metadata": {},
   "source": [
    "**2.2 MAE — Mean Absolute Error**\n",
    "\n",
    "***Used for: Regression with outliers***\n",
    "\n",
    "***Why: Robust to extreme values (error is linear).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685be916",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_reg, y_pred_reg)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a758a385",
   "metadata": {},
   "source": [
    "\n",
    "**2.3 Huber Loss**\n",
    "\n",
    "***Used for: When data has medium noise***\n",
    "\n",
    "***Why: Smooth like MSE for small errors but robust like MAE for big errors.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    is_small = np.abs(error) <= delta\n",
    "    small_loss = 0.5 * (error ** 2)\n",
    "    big_loss = delta * (np.abs(error) - 0.5 * delta)\n",
    "    return np.where(is_small, small_loss, big_loss).mean()\n",
    "\n",
    "huber = huber_loss(y_reg, y_pred_reg)\n",
    "print(\"Huber Loss:\", huber)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745c8b81",
   "metadata": {},
   "source": [
    "***3. Classification Loss Functions Example (Log Loss, Hinge, Squared Hinge)***\n",
    "\n",
    "Create a binary classification dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clf, y_clf = make_classification(\n",
    "    n_samples=200, n_features=4, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62100abf",
   "metadata": {},
   "source": [
    "**3.1 Log Loss (Binary Cross Entropy)**\n",
    "\n",
    "***Used for: Logistic Regression & probability-based models***\n",
    "\n",
    "***Why: Measures how well predicted probabilities match labels.***\n",
    "\n",
    "Train logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_clf, y_clf)\n",
    "\n",
    "probs = log_model.predict_proba(X_clf)\n",
    "logloss = log_loss(y_clf, probs)\n",
    "print(\"Log Loss:\", logloss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7663908",
   "metadata": {},
   "source": [
    "**3.2 Hinge Loss**\n",
    "\n",
    "***Used for: SVM classifiers***\n",
    "***Why: Encourages a margin between classes.***\n",
    "\n",
    "Train linear SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d50117",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(X_clf, y_clf)\n",
    "\n",
    "# Hinge loss needs decision function\n",
    "decision = svm.decision_function(X_clf)\n",
    "hinge = hinge_loss(y_clf, decision)\n",
    "print(\"Hinge Loss:\", hinge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3999b66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss Function</th>\n",
       "      <th>Use Case</th>\n",
       "      <th>Why Use It</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSE</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Penalizes large errors strongly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAE</td>\n",
       "      <td>Regression (Outliers)</td>\n",
       "      <td>Robust to large outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Huber</td>\n",
       "      <td>Regression (Mixed Noise)</td>\n",
       "      <td>Mix of robust + smooth behavior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Log Loss</td>\n",
       "      <td>Binary Classification</td>\n",
       "      <td>Compares predicted probability vs true label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hinge Loss</td>\n",
       "      <td>SVM Margin-Based Classification</td>\n",
       "      <td>For margin-maximizing classifiers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Squared Hinge Loss</td>\n",
       "      <td>Hard Margin Classification</td>\n",
       "      <td>Stronger penalty than hinge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Loss Function                         Use Case  \\\n",
       "0                 MSE                       Regression   \n",
       "1                 MAE            Regression (Outliers)   \n",
       "2               Huber         Regression (Mixed Noise)   \n",
       "3            Log Loss            Binary Classification   \n",
       "4          Hinge Loss  SVM Margin-Based Classification   \n",
       "5  Squared Hinge Loss       Hard Margin Classification   \n",
       "\n",
       "                                     Why Use It  \n",
       "0               Penalizes large errors strongly  \n",
       "1                      Robust to large outliers  \n",
       "2               Mix of robust + smooth behavior  \n",
       "3  Compares predicted probability vs true label  \n",
       "4             For margin-maximizing classifiers  \n",
       "5                   Stronger penalty than hinge  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Loss Function\": [\n",
    "        \"MSE\", \"MAE\", \"Huber\", \n",
    "        \"Log Loss\", \"Hinge Loss\", \"Squared Hinge Loss\"\n",
    "    ],\n",
    "    \"Use Case\": [\n",
    "        \"Regression\",\n",
    "        \"Regression (Outliers)\",\n",
    "        \"Regression (Mixed Noise)\",\n",
    "        \"Binary Classification\",\n",
    "        \"SVM Margin-Based Classification\",\n",
    "        \"Hard Margin Classification\"\n",
    "    ],\n",
    "    \"Why Use It\": [\n",
    "        \"Penalizes large errors strongly\",\n",
    "        \"Robust to large outliers\",\n",
    "        \"Mix of robust + smooth behavior\",\n",
    "        \"Compares predicted probability vs true label\",\n",
    "        \"For margin-maximizing classifiers\",\n",
    "        \"Stronger penalty than hinge\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "summary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
